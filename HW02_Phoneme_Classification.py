# -*- coding: utf-8 -*-
"""HW2-Phoneme Classification

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1i7CiUZ-g5T1c62x5av72F_73IfccLqDo

# **Homework 2-1 Phoneme Classification**

## The DARPA TIMIT Acoustic-Phonetic Continuous Speech Corpus (TIMIT)
The TIMIT corpus of reading speech has been designed to provide speech data for the acquisition of acoustic-phonetic knowledge and for the development and evaluation of automatic speech recognition systems.

This homework is a multiclass classification task, 
we are going to train a deep neural network classifier to predict the phonemes for each frame from the speech corpus TIMIT.

link: https://academictorrents.com/details/34e2b78745138186976cbc27939b1b34d18bd5b3

## Preparing Data
Load the training and testing data from the `.npy` file (NumPy array).
"""

!pip install torchensemble
from torchensemble.utils.io import load
from torchensemble.bagging import BaggingClassifier
from torchensemble.voting import VotingClassifier

!gdown --id '1Rt7FNl9GQKBd0YknlupdAz8f62rDztmu' --output data.zip
!unzip data.zip
!ls
#this model looks like overfit

import numpy as np

print('Loading data ...')

data_root='./timit_11/'
train = np.load(data_root + 'train_11.npy')
train_label = np.load(data_root + 'train_label_11.npy')
test = np.load(data_root + 'test_11.npy')

print('Size of training data: {}'.format(train.shape))
print('Size of testing data: {}'.format(test.shape))

"""## Create Dataset"""

import torch
from torch.utils.data import Dataset

class TIMITDataset(Dataset):
    def __init__(self, X, y=None):
        self.data = torch.from_numpy(X).float()
        if y is not None:
            y = y.astype(np.int)
            self.label = torch.LongTensor(y)
        else:
            self.label = None

    def __getitem__(self, idx):
        if self.label is not None:
            return self.data[idx], self.label[idx]
        else:
            return self.data[idx]

    def __len__(self):
        return len(self.data)

"""Split the labeled data into a training set and a validation set, you can modify the variable `VAL_RATIO` to change the ratio of validation data."""

VAL_RATIO = 1e-3

percent = int(train.shape[0] * (1 - VAL_RATIO))
train_x, train_y, val_x, val_y = train[:percent], train_label[:percent], train[percent:], train_label[percent:]
print('Size of training set: {}'.format(train_x.shape))
print('Size of validation set: {}'.format(val_x.shape))

"""Create a data loader from the dataset, feel free to tweak the variable `BATCH_SIZE` here."""

BATCH_SIZE = 100

from torch.utils.data import DataLoader

train_set = TIMITDataset(train_x, train_y)
val_set = TIMITDataset(val_x, val_y)
train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True) #only shuffle the training data
val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)

"""Cleanup the unneeded variables to save memory.<br>

**notes: if you need to use these variables later, then you may remove this block or clean up unneeded variables later<br>the data size is quite huge, so be aware of memory usage in colab**
"""

import gc

del train, train_label, train_x, train_y, val_x, val_y
gc.collect()

"""## Create Model

Define model architecture, you are encouraged to change and experiment with the model architecture.
"""

import torch
import torch.nn as nn

class Classifier(nn.Module):
    def __init__(self):
        super(Classifier, self).__init__()
        self.rnn = nn.LSTM(     
            input_size=39,      
            hidden_size=256,
            bidirectional=True,     
            num_layers=2,
            dropout = 0.4,       
            batch_first=True,   
        )

        
        self.layer1 = nn.Linear(512, 256)
        self.layer2 = nn.Linear(256, 128)
        self.out = nn.Linear(128, 39) 

        self.act_fn = nn.ReLU()

        self.batchnorm1 = nn.BatchNorm1d(256)
        self.batchnorm2 = nn.BatchNorm1d(128)

        self.dropout = nn.Dropout(p = 0.4)

        self.criterion = nn.CrossEntropyLoss()

    def forward(self, x):
        x = x.view(-1, 11, 39)
        
        x, (h_n, h_c) = self.rnn(x, None)
        
        x = self.layer1(x[:, -1, :])
        x = self.batchnorm1(x)
        x = self.act_fn(x)
        x = self.dropout(x)

        x = self.layer2(x)
        x = self.batchnorm2(x)
        x = self.act_fn(x)
        x = self.dropout(x)

        x = self.out(x)
        
        return x
    
    def cal_loss(self, outputs, labels):
        return self.criterion(outputs, labels)

"""## Training"""

#check device
def get_device():
  return 'cuda' if torch.cuda.is_available() else 'cpu'

"""Fix random seeds for reproducibility."""

# fix random seed
def same_seeds(seed):
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)  
    np.random.seed(seed)  
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True

"""Feel free to change the training parameters here."""

# fix random seed for reproducibility
same_seeds(0)

# get device 
device = get_device()
print(f'DEVICE: {device}')

# training parameters
num_epoch = 15               # number of training epoch
learning_rate = 0.0003       # learning rate
n_estimator = 15

# the path where checkpoint saved
model_path = './model'

# create model, define a loss function, and optimizer
model = VotingClassifier(
    estimator=Classifier,
    n_estimators=n_estimator,
    cuda=True
)

model.set_optimizer("Adam", lr=learning_rate, weight_decay=1e-5)
model.fit( train_loader=train_loader,
      epochs=num_epoch,
      test_loader=val_loader,
      save_model=True,
      save_dir=model_path)

"""**Load Function**

## Testing

Create a testing dataset, and load model from the saved checkpoint.
"""

# create testing dataset
test_set = TIMITDataset(test, None)
test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)

# create model and load weights from checkpoint
del model
model = VotingClassifier(
    estimator=Classifier,
    n_estimators=n_estimator,
    cuda=True
)


load(model, save_dir=model_path, logger=None)

"""Make prediction."""

predict = []
model.eval() # set the model to evaluation mode
with torch.no_grad():
    for i, data in enumerate(test_loader):
        inputs = data
        inputs = inputs.to(device)
        outputs = model(inputs)
        _, test_pred = torch.max(outputs, 1) # get the index of the class with the highest probability

        for y in test_pred.cpu().numpy():
            predict.append(y)

"""Write prediction to a CSV file.

After finish running this block, download the file `prediction.csv` from the files section on the left-hand side and submit it to Kaggle.
"""

with open('prediction.csv', 'w') as f:
    f.write('Id,Class\n')
    for i, y in enumerate(predict):
        f.write('{},{}\n'.format(i, y))

"""**reference:https://github.com/xuyxu/Ensemble-Pytorch**"""